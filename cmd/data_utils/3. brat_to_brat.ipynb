{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "altered-parking",
   "metadata": {},
   "source": [
    "# Brat to Brat conversion\n",
    "\n",
    "Converts a Brat dataset into another Brat dataset by converting a set of entity tags and attributes to another set.\n",
    "\n",
    "Do not consider relations nor events (will not be copied into the converted dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "widespread-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "naughty-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, isdir, join\n",
    "from os import makedirs, walk, listdir\n",
    "from shutil import rmtree, copy\n",
    "import re\n",
    "from random import randint\n",
    "\n",
    "from brat_conversion_schemas import BRAT_CONVERSION_SCHEMAS\n",
    "\n",
    "# Directory containing the original Brat annotations\n",
    "INDIR = '../../data/release/train2021'\n",
    "\n",
    "# Name of the conversion schema to be used (among those listed in BRAT_CONVERSION_SCHEMAS)\n",
    "CONVERSION_SCHEMA = 'sosy_and_pathologies_with_aggregated_attributes'\n",
    "\n",
    "# Directory containing the output (Brat annotations after mapping)\n",
    "OUTDIR = f'../../data/work/brat_mapping/{CONVERSION_SCHEMA}'\n",
    "\n",
    "assert(isdir(INDIR))\n",
    "if isdir(OUTDIR):\n",
    "    rmtree(OUTDIR)\n",
    "makedirs(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "steady-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONVERSION_SCHEMA not in BRAT_CONVERSION_SCHEMAS:\n",
    "    raise KeyError(f'Unknown schema {CONVERSION_SCHEMA}')\n",
    "    \n",
    "conversion_schema = BRAT_CONVERSION_SCHEMAS[CONVERSION_SCHEMA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "technical-latvia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'sosy_and_pathologies_with_aggregated_attributes',\n",
       " 'desc': '\"sosy\" and \"pathology\" are NOT grouped together into \"sosypath\", they are left alone, and each\"assertion\" attribute leads to an entity sosy_nonfactual and pathology_nonfactual. The result is \"sosy\" (no assertion attribute), \"sosy_nonfactual\",\"pathologie\" (no assertion attribute), \"pathologie_nonfactual\"',\n",
       " 'skip_if_absent': True,\n",
       " 'mapping': {('sosy', '', ''): ('sosy', '', ''),\n",
       "  ('sosy', 'assertion', 'absent'): ('sosy_absent', '', ''),\n",
       "  ('sosy', 'assertion', 'hypothétique'): ('sosy_hypothetique', '', ''),\n",
       "  ('sosy', 'assertion', 'non-associé'): ('sosy_non_associe', '', ''),\n",
       "  ('pathologie', '', ''): ('pathologie', '', ''),\n",
       "  ('pathologie', 'assertion', 'absent'): ('pathologie_absent', '', ''),\n",
       "  ('pathologie', 'assertion', 'hypothétique'): ('pathologie_hypothetique',\n",
       "   '',\n",
       "   ''),\n",
       "  ('pathologie', 'assertion', 'non-associé'): ('pathologie_non_associe',\n",
       "   '',\n",
       "   ''),\n",
       "  ('sosy', '*', '*'): ('sosy', '', ''),\n",
       "  ('pathologie', '*', '*'): ('pathologie', '', '')}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversion_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "miniature-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = conversion_schema['mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hydraulic-brush",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_REGEX = re.compile('^(T\\d+)\\t([^ ]+) ([^\\t]+)\\t(.*)$')\n",
    "ATTRIBUTE_REGEX = re.compile('(A\\d+)\\t([^ ]+) (T\\d+) ?(.*)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "central-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cmp_to_key\n",
    "def sort_triplets(x, y):\n",
    "    if x[0] == '*':\n",
    "        return 1\n",
    "    elif y[0] == '*':\n",
    "        return -1\n",
    "    elif x[0] != y[0]:\n",
    "        return x[0] < y[0]\n",
    "    else:\n",
    "        if x[1] == '*':\n",
    "            return 1\n",
    "        elif y[1] == '*':\n",
    "            return -1\n",
    "        elif x[1] != y[1]:\n",
    "            return x[1] < y[1]\n",
    "        else:\n",
    "            if x[2] == '*':\n",
    "                return 1\n",
    "            elif y[2] == '*':\n",
    "                return -1\n",
    "            elif x[2] != y[2]:\n",
    "                return x[2] < y[2]\n",
    "            else:\n",
    "                return 0\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "swiss-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in listdir(INDIR):\n",
    "    if filename.endswith('.ann'):\n",
    "        annpath = join(INDIR, filename)\n",
    "        txtpath = join(INDIR, filename[:-4] + '.txt')\n",
    "        # copy text file\n",
    "        assert(isfile(txtpath))\n",
    "        copy(txtpath, OUTDIR)\n",
    "        outannpath = join(OUTDIR, filename)\n",
    "        \n",
    "        entities = {}\n",
    "        attributes = {}\n",
    "        attribute_ids = set()\n",
    "        \n",
    "        # Read Brat annotation informations\n",
    "        with open(annpath, 'r', encoding='utf-8') as f_ann:\n",
    "            for line in f_ann:\n",
    "                line = line.strip()\n",
    "                # parse entity\n",
    "                entity_match = ENTITY_REGEX.match(line)\n",
    "                if entity_match is not None:\n",
    "                    t_id = entity_match.group(1)\n",
    "                    t_type = entity_match.group(2)\n",
    "                    t_offsets = entity_match.group(3)\n",
    "                    t_text = entity_match.group(4)\n",
    "                    entities[t_id] = (t_type, t_offsets, t_text)\n",
    "                    continue\n",
    "                # parse attribute   \n",
    "                attribute_match = ATTRIBUTE_REGEX.match(line)\n",
    "                if attribute_match is not None:\n",
    "                    a_id = attribute_match.group(1)\n",
    "                    a_key = attribute_match.group(2)\n",
    "                    t_id = attribute_match.group(3)\n",
    "                    if len(attribute_match.groups()) > 3:\n",
    "                        a_value = attribute_match.group(4)\n",
    "                    else:\n",
    "                        a_value = None\n",
    "                    t_attributes = attributes.get(t_id, [])\n",
    "                    t_attributes.append((a_id, a_key, a_value))\n",
    "                    attributes[t_id] = t_attributes\n",
    "                    attribute_ids.add(a_id)\n",
    "                    \n",
    "        # Parse the results and convert\n",
    "        with open(outannpath, 'w', encoding='utf-8') as f_out:\n",
    "            for t_id, (t_type, t_offset, t_text) in entities.items():\n",
    "                found = None\n",
    "                # the entity type can be mapped to \"*\" or to its name\n",
    "                # (starting by the most specific)\n",
    "                for t in (t_type, '*'):\n",
    "                    # if the entity has attributes\n",
    "                    if t_id in attributes:\n",
    "                        attribute_loop = [\n",
    "                            (\"*\", \"*\")\n",
    "                        ]\n",
    "                        for (_, a_key, a_value) in attributes[t_id]:\n",
    "                            attribute_loop.extend([\n",
    "                                (a_key, \"*\"),\n",
    "                                (a_key, a_value)\n",
    "                            ])\n",
    "                    # if the entity has no attribute, it can be mapped to \"no attribute\"\n",
    "                    # or to any attribute\n",
    "                    else:\n",
    "                        attribute_loop = [\n",
    "                            (\"\", \"\"),\n",
    "                            (\"*\", \"*\")\n",
    "                        ]\n",
    "                    # sort attribute mapping from most specific to less specific\n",
    "                    attribute_loop = sorted(attribute_loop, key=cmp_to_key(sort_triplets))\n",
    "\n",
    "                    for (k, v) in attribute_loop:\n",
    "                        if (t, k, v) in mapping:\n",
    "                            found = (t, k, v), mapping[(t, k, v)]\n",
    "                            break\n",
    "                    if found is not None:\n",
    "                        break\n",
    "                # to be converted!\n",
    "                if found is not None:\n",
    "                    (old_t, old_k, old_v), (new_t, new_k, new_v) = found\n",
    "                    if new_t is None:\n",
    "                        new_t = t_type\n",
    "                    if new_k is None:\n",
    "                        new_k = old_k\n",
    "                    if new_v is None:\n",
    "                        new_v = old_v\n",
    "                    f_out.write(f'{t_id}\\t{new_t} {t_offset}\\t{t_text}\\n')\n",
    "                    # choose random attribute id\n",
    "                    a_id = randint(0, 10000)\n",
    "                    if new_k != '':\n",
    "                        # reproduce all attributes\n",
    "                        if new_k == '*':\n",
    "                            for (a_id, a_key, a_value) in attributes.get(t_id, []):\n",
    "                                f_out.write(f'{a_id}\\t{a_key} {t_id} {a_value}\\n')\n",
    "                        elif new_v == '*':\n",
    "                            for (a_id, a_key, a_value) in attributes.get(t_id, []):\n",
    "                                if a_key == new_k:\n",
    "                                    f_out.write(f'{a_id}\\t{a_key} {t_id} {a_value}\\n')\n",
    "                        else:\n",
    "                            # build new attribute\n",
    "                            while 'A' + str(a_id) in attribute_ids:\n",
    "                                a_id = randint(0, 10000)\n",
    "                            f_out.write(f'A{a_id}\\t{new_k} {t_id} {new_v}\\n')\n",
    "        # Write description as a README\n",
    "        with open(join(OUTDIR, 'README'), 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write('# ' + conversion_schema['name'] + '\\n\\n')\n",
    "            f_out.write(conversion_schema['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "laughing-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/work/brat_mapping/sosy_and_pathologies_with_aggregated_attributes'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-praise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp38",
   "language": "python",
   "name": "nlp38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
